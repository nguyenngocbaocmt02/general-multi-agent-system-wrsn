alg_params:
	gumbel_softmax: False
	epsilon_softmax: False
	softmax_eps: None
	episodic: False
	cuda: True
	grad_clip_eps: 1.0
	save_model_freq: 40
	replay_warmup: 0
	policy_lrate: 0.0001
	value_lrate: 0.0001
	mixer_lrate: None
	target: True
	target_lr: 0.1
	entr: 0.001
	max_steps: 2
	batch_size: 32
	replay: True
	replay_buffer_size: 5000.0
	agent_type: rnn
	agent_id: True
	shared_params: True
	layernorm: True
	mixer: False
	gaussian_policy: False
	LOG_STD_MIN: 0.0
	LOG_STD_MAX: 0.5
	fixed_policy_std: 1.0
	hid_activation: relu
	init_type: normal
	init_std: 0.1
	action_enforcebound: True
	double_q: True
	clip_c: 1.0
	gamma: 0.99
	hid_size: 64
	continuous: True
	normalize_advantages: False
	train_episodes_num: 1
	behaviour_update_freq: 60
	target_update_freq: 120
	policy_update_epochs: 1
	value_update_epochs: 10
	mixer_update_epochs: None
	reward_normalisation: True
	eval_freq: 20
	num_eval_episodes: 1
	action_scale: None
	action_bias: None
	agent_num: 1
	obs_size: 100
	action_dim: 6
env_params:
	node_capacity: 10800
	node_threshold: 540
	mc_specification: {'mc_capacity': 108000, 'mc_threshold': 0, 'mc_velocity': 5, 'mc_pm': 1, 'mc_charging_range': 100, 'mc_alpha': 4500, 'mc_beta': 30, 'epsilon': 1e-10}
	node_phy_spe: {'node_com_range': 100, 'node_prob_gp': 0.8, 'package_size': 400.0, 'er': 0.0001, 'et': 5e-05, 'efs': 1e-08, 'emp': 1.3e-12}
	scenario: ./environments/data/test.yaml
	episode_limit: 240
	action_scale: None
	action_bias: None
	mode: decentralized
	reset_action: True
	seed: 0
	n_mcs: 2
	degree: 10
